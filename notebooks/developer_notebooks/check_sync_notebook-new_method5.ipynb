{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check sync process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local configuration file found !!, no need to run the configuration (unless configuration has changed)\n"
     ]
    }
   ],
   "source": [
    "from scripts.conf_file_finding import try_find_conf_file\n",
    "try_find_conf_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MiniVR-8\\miniconda3\\envs\\u19_pipeline_python_env\\Lib\\site-packages\\datajoint\\plugin.py:4: UserWarning: Module scripts was already imported from None, but c:\\experiments\\u19-pipeline-python is being added to sys.path\n",
      "  import pkg_resources\n",
      "[2025-10-29 18:10:17,984][INFO]: DataJoint 0.14.6 connected to u19prod@datajoint00.pni.princeton.edu:3306\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal as sp\n",
    "\n",
    "\n",
    "from u19_pipeline.ephys_pipeline import ephys_element, probe_element, get_session_directory, get_ephys_root_data_dir\n",
    "# import u19_pipeline.ephys_sync as ephys\n",
    "import u19_pipeline.acquisition as acquisition\n",
    "import datajoint as dj\n",
    "\n",
    "from element_interface.utils import find_full_path\n",
    "\n",
    "import u19_pipeline.utils.DemoReadSGLXData.readSGLX as readSGLX\n",
    "import u19_pipeline.utils.ephys_utils as ephys_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Behavior data\n",
    "behavior = dj.create_virtual_module('behavior', 'u19_behavior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shift_vector(synced_time_vector, behavior_time_vector, base_size=40,initial_sample=0, samples_shift=100):\n",
    "\n",
    "\n",
    "    diff_size = np.abs(synced_time_vector.shape[0] - behavior_time_vector.shape[0])\n",
    "\n",
    "    baseline_diff = synced_time_vector[initial_sample:base_size] - behavior_time_vector[initial_sample:base_size] \n",
    "\n",
    "    while(1):\n",
    "\n",
    "        base_greater = np.where(baseline_diff >0)\n",
    "        base_greater = base_greater[0]\n",
    "\n",
    "        if base_greater.shape[0] > 0 and synced_time_vector.shape[0] < behavior_time_vector.shape[0]:\n",
    "            idx_first = base_greater[0]\n",
    "            time_bef = behavior_time_vector[idx_first] - behavior_time_vector[idx_first-1]\n",
    "\n",
    "\n",
    "            #print('idx_first', idx_first)\n",
    "            #print('time_bef', time_bef)\n",
    "            #print('synced_time_vector', synced_time_vector[idx_first-2:idx_first+2])\n",
    "            #print('behavior_time_vector', behavior_time_vector[idx_first-2:idx_first+2])\n",
    "\n",
    "            synced_time_vector = np.insert(synced_time_vector, idx_first, synced_time_vector[idx_first-1]+time_bef)\n",
    "\n",
    "            #print('synced_time_vector', synced_time_vector[idx_first-2:idx_first+2])\n",
    "\n",
    "            baseline_diff = synced_time_vector[initial_sample:base_size] - behavior_time_vector[initial_sample:base_size]\n",
    "        else:\n",
    "            break \n",
    "\n",
    "\n",
    "    median_diff = np.median(baseline_diff)\n",
    "    max_diff = np.median(baseline_diff)+0.007\n",
    "    min_diff = np.median(baseline_diff)-0.007\n",
    "\n",
    "\n",
    "    vec_shift = np.zeros((synced_time_vector.shape[0]-base_size), dtype=int)\n",
    "    for i in range(synced_time_vector.shape[0]-base_size):\n",
    "\n",
    "\n",
    "        idx_start = initial_sample+i+1\n",
    "        idx_end = base_size+i+1\n",
    "\n",
    "        this_bt = behavior_time_vector[idx_start:idx_end]\n",
    "        this_iv = synced_time_vector[idx_start:idx_end]\n",
    "\n",
    "        median_ori = np.median(this_iv-this_bt)\n",
    "\n",
    "        if median_ori >= min_diff and median_ori < max_diff:\n",
    "            vec_shift[i] = 0\n",
    "            sign = 0\n",
    "        elif median_ori < min_diff:\n",
    "            sign = 1\n",
    "        else:\n",
    "            sign = -1\n",
    "            \n",
    "        new_sign = sign\n",
    "        if sign != 0:\n",
    "\n",
    "            for j in range(1, samples_shift):\n",
    "\n",
    "                if new_sign == 1:\n",
    "                    if idx_end+j > synced_time_vector.shape[0]:\n",
    "                        this_iv = synced_time_vector[idx_start+j:]\n",
    "                        this_bt = this_bt[:-1]\n",
    "                    else:\n",
    "                        this_iv = synced_time_vector[idx_start+j:idx_end+j]\n",
    "                else:\n",
    "                    this_iv = synced_time_vector[idx_start-j:idx_end-j]\n",
    "                \n",
    "                median_now = np.median(this_iv-this_bt)\n",
    "                \n",
    "                if median_now >= min_diff and median_now < max_diff:\n",
    "                    vec_shift[i] = j*new_sign\n",
    "                    break\n",
    "                elif median_now < min_diff:\n",
    "                    new_sign = 1\n",
    "                else:\n",
    "                    new_sign = -1\n",
    "\n",
    "                if new_sign != sign:\n",
    "                    if np.abs(median_ori-median_diff) < np.abs(median_now-median_diff):\n",
    "                        vec_shift[i] = 0\n",
    "                    else:\n",
    "                        vec_shift[i] = j*sign                \n",
    "                    break\n",
    "                    \n",
    "                if j == samples_shift-1:\n",
    "                    #print('Extreme case !!!')\n",
    "                    vec_shift[i] = j*sign\n",
    "\n",
    "    new_synced_time_vector = synced_time_vector.copy()\n",
    "    mid_point = int(initial_sample+base_size/2)\n",
    "    for i in range(vec_shift.shape[0]):\n",
    "        new_synced_time_vector[mid_point+i] = synced_time_vector[mid_point+i+vec_shift[i]]\n",
    "    \n",
    "    idx_end = mid_point+vec_shift.shape[0]-1\n",
    "    for i in range(idx_end, new_synced_time_vector.shape[0]):\n",
    "        if i+vec_shift[-1] < new_synced_time_vector.shape[0]:\n",
    "            new_synced_time_vector[i] = synced_time_vector[i+vec_shift[-1]]\n",
    "\n",
    "    max_shift = np.max(np.abs(vec_shift))\n",
    "\n",
    "    if max_shift > diff_size and diff_size != 1:\n",
    "        print('max_shift', max_shift)\n",
    "        print('diff_size', diff_size)\n",
    "        raise ValueError('more max shift than diff size')\n",
    "\n",
    "    return new_synced_time_vector, vec_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_shifted_sync_vector(synced_time_vector, behavior_time_vector, vec_shift, initial_sample=0, base_size=40):\n",
    "\n",
    "    mid_point = int(initial_sample+base_size/2)\n",
    "    new_synced_time_vector = synced_time_vector.copy()\n",
    "\n",
    "    diff_vec_shift = np.diff(vec_shift)\n",
    "    where_insert_iteration = np.where(diff_vec_shift != 0)\n",
    "    where_insert_iteration = where_insert_iteration[0]\n",
    "    #print('where_insert_iteration', where_insert_iteration)\n",
    "\n",
    "    find = np.full(4, 1)\n",
    "    correlation_result = np.correlate(diff_vec_shift == 0, find)\n",
    "    consecutive_zeros = np.flatnonzero(correlation_result == 4)\n",
    "\n",
    "    index_shift = 0\n",
    "    index_borrow_virmen = list()\n",
    "    for i in range(len(where_insert_iteration)):\n",
    "        stable_parts = np.where(consecutive_zeros > where_insert_iteration[index_shift])\n",
    "        stable_parts = stable_parts[0]\n",
    "\n",
    "        if stable_parts.shape[0] > 0:\n",
    "            index_borrow_virmen.append([where_insert_iteration[index_shift], consecutive_zeros[stable_parts[0]]])\n",
    "            next_borrow_start = np.where(where_insert_iteration > consecutive_zeros[stable_parts[0]])\n",
    "            next_borrow_start = next_borrow_start[0]\n",
    "            if next_borrow_start.shape[0] > 0:\n",
    "                index_shift = next_borrow_start[0]\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print('Extreme case diff vec shift')\n",
    "\n",
    "            index_borrow_virmen.append([where_insert_iteration[index_shift], diff_vec_shift.shape[0]])\n",
    "            print('index_borrow_virmen', index_borrow_virmen)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    #print('index_borrow_virmen', index_borrow_virmen)\n",
    "\n",
    "    #for i in range(where_insert_iteration.shape[0]):\n",
    "    index_diff_e  = -1\n",
    "    for i in range(len(index_borrow_virmen)):\n",
    "\n",
    "        if index_diff_e > index_borrow_virmen[i][0]+mid_point:\n",
    "            continue\n",
    "\n",
    "        for j in range(100):\n",
    "\n",
    "            index_diff_s = index_borrow_virmen[i][0]+mid_point-1\n",
    "            index_diff_e = index_borrow_virmen[i][1]+mid_point+1\n",
    "\n",
    "            if j>=1:\n",
    "                index_diff_s = index_diff_s-1\n",
    "            if j >1:\n",
    "                index_diff_e = index_diff_e+j-1\n",
    "\n",
    "            #print('index_diff_vec', index_diff_s, index_diff_e)\n",
    "            #print('diff_vec_shift[index_diff] ', diff_vec_shift[index_diff_vec-1:index_diff_vec+1] )\n",
    "\n",
    "            #print('behavior_time_vector', behavior_time_vector[index_diff_s-1:index_diff_e+2])\n",
    "            #print('new_synced_time_vector2', new_synced_time_vector2[index_diff_s-1:index_diff_e+2])\n",
    "\n",
    "            time_iteration = behavior_time_vector[index_diff_s:index_diff_e+1] - behavior_time_vector[index_diff_s]\n",
    "            #print('time_iteration', time_iteration)\n",
    "\n",
    "            new_synced_time_vector[index_diff_s:index_diff_e+1] = np.repeat(new_synced_time_vector[index_diff_s], index_diff_e-index_diff_s+1) +\\\n",
    "                time_iteration\n",
    "            \n",
    "            check_diff = np.diff(new_synced_time_vector[index_diff_s:index_diff_e+2])\n",
    "            idx_back_time = np.where(check_diff <= 0.0005)\n",
    "            idx_back_time = idx_back_time[0]\n",
    "            if idx_back_time.shape[0] == 0:\n",
    "                break\n",
    "        \n",
    "        #print('new_synced_time_vector2', new_synced_time_vector2[index_diff_s-1:index_diff_e+2])\n",
    "        #print('diff new_synced_time_vector2', np.diff(new_synced_time_vector2[index_diff_s-1:index_diff_e+2]))\n",
    "        #print('diff new_synced_time_vector2', np.diff(behavior_time_vector[index_diff_s-1:index_diff_e+2]))\n",
    "\n",
    "\n",
    "\n",
    "    return new_synced_time_vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_sync_vector_greater(sync_time_vector, behavior_time_vector):\n",
    "\n",
    "    new_sync_time_vector = sync_time_vector.copy()\n",
    "\n",
    "    diff_vecs = (new_sync_time_vector - behavior_time_vector[:new_sync_time_vector.shape[0]])\n",
    "\n",
    "    idx_plus = np.where(diff_vecs > 0)\n",
    "    idx_plus = idx_plus[0]\n",
    "\n",
    "    break_points = np.where(np.diff(idx_plus) != 1)[0] + 1\n",
    "    grouped_sections = np.split(idx_plus, break_points)\n",
    "\n",
    "    for i in range(len(grouped_sections)):\n",
    "\n",
    "        if grouped_sections[i].shape[0] > 0:\n",
    "\n",
    "            idx_start = grouped_sections[i][0]-1\n",
    "            idx_end = grouped_sections[i][-1]\n",
    "\n",
    "            time_vector = behavior_time_vector[idx_start:idx_end+1]-behavior_time_vector[idx_start]\n",
    "            new_sync_time_vector[idx_start:idx_end+1] =\\\n",
    "                np.repeat(new_sync_time_vector[idx_start], idx_end-idx_start+1) + time_vector\n",
    "        \n",
    "    return new_sync_time_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_last_part_sync_vec(sync_time_vector, behavior_time_vector):\n",
    "\n",
    "    new_sync_time_vector = sync_time_vector.copy()\n",
    "    \n",
    "    diff_size = behavior_time_vector.shape[0] - new_sync_time_vector.shape[0]\n",
    "\n",
    "    if diff_size > 0:\n",
    "        diff_size = diff_size+1\n",
    "\n",
    "        last_part_bt = behavior_time_vector[-diff_size:]- behavior_time_vector[-diff_size]\n",
    "        insert_part = np.repeat(new_sync_time_vector[-1], diff_size) + last_part_bt\n",
    "        #print('last_part_bt',last_part_bt)\n",
    "\n",
    "        #print('insert_part',insert_part)\n",
    "\n",
    "        new_sync_time_vector = np.append(new_sync_time_vector, insert_part[1:])\n",
    "    \n",
    "    return new_sync_time_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_iter_vector(synced_iteration_vector, synced_time_vector, original_time_vector, nidq_sampling_rate):\n",
    "\n",
    "    new_synced_iteration_vector = synced_iteration_vector.copy()\n",
    "\n",
    "    ori_shape = original_time_vector.shape[0]\n",
    "    new_shape = synced_time_vector.shape[0]\n",
    "    diff_iter_times_idx = original_time_vector - synced_time_vector[:ori_shape]\n",
    "\n",
    "    first_iter = synced_iteration_vector[0]\n",
    "    for i in range(diff_iter_times_idx.shape[0]):\n",
    "        if diff_iter_times_idx[i] != 0:\n",
    "            new_synced_iteration_vector[i] = first_iter+int(synced_time_vector[i]*nidq_sampling_rate)\n",
    "\n",
    "    if ori_shape != new_shape:\n",
    "        last_iterations =  np.round(first_iter+synced_time_vector[ori_shape:]*nidq_sampling_rate)\n",
    "        new_synced_iteration_vector = np.append(new_synced_iteration_vector, last_iterations)\n",
    "\n",
    "    return new_synced_iteration_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_evaluation_process2(synced_time_vector, behavior_time_vector):\n",
    "\n",
    "    status = 1\n",
    "    diff_vector = synced_time_vector - behavior_time_vector[:synced_time_vector.shape[0]]\n",
    "    num_iter = diff_vector.shape[0]\n",
    "\n",
    "    #max_diff = max(diff_vector)\n",
    "    median_general = np.median(diff_vector)\n",
    "\n",
    "    num_div= 10\n",
    "    median_diff_percent = np.empty([num_div])\n",
    "    median_diff_abs = np.empty([num_div])\n",
    "    for j in range(num_div):\n",
    "        start_iter = int(j*num_iter/num_div)\n",
    "        end_iter = int((j+1)*num_iter/num_div)\n",
    "        median_diff_percent[j] = (np.median(diff_vector[start_iter:end_iter])-median_general)*100/median_general\n",
    "        median_diff_abs[j] = (np.median(diff_vector[start_iter:end_iter])-median_general)\n",
    "\n",
    "\n",
    "    if np.max(np.abs(median_diff_abs)) < 0.005:\n",
    "        pass\n",
    "    else:\n",
    "        status = 0\n",
    "        print(median_general)\n",
    "        plt.plot(median_diff_abs)\n",
    "\n",
    "    return status\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'recording_id': np.int64(500)},\n",
       " {'recording_id': np.int64(501)},\n",
       " {'recording_id': np.int64(502)},\n",
       " {'recording_id': np.int64(503)},\n",
       " {'recording_id': np.int64(504)},\n",
       " {'recording_id': np.int64(505)},\n",
       " {'recording_id': np.int64(506)},\n",
       " {'recording_id': np.int64(507)}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording_query = \"recording_id >= 500\"\n",
    "recording = dj.create_virtual_module('recording', 'u19_recording')\n",
    "recording_keys = (recording.Recording & recording_query).fetch('recording_id', as_dict=True, order_by='recording_id')\n",
    "\n",
    "\n",
    "session_fields = ['subject_fullname', 'session_date', 'session_number']\n",
    "session_keys = (recording.Recording.BehaviorSession & recording_query).fetch(*session_fields, as_dict=True, order_by='recording_id')\n",
    "\n",
    "\n",
    "recording_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronize ePhys and Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the tables, using the nidaq file produced by spikeGLX. This is done automatically and produces a record of VR iteration numbers measured in the time of the ePhys setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read nidaq file and behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\cup.pni.princeton.edu\\braininit\\Data\\Raw\\electrophysiology\\jyanar\\jyanar_ya054\\20251011_g0\\ya054_20251011_g0\n",
      "nChan: 4, nFileSamp: 111367791\n",
      "mode deduction:  pulse_signal\n",
      "Synced perfectly xxxxxxxxxxxxxx\n",
      "\\\\cup.pni.princeton.edu\\braininit\\Data\\Raw\\electrophysiology\\jyanar\\jyanar_ya054\\20251012_g0\\ya054_20251012_g0\n",
      "nChan: 4, nFileSamp: 113704412\n",
      "mode deduction:  pulse_signal\n",
      "Synced perfectly xxxxxxxxxxxxxx\n",
      "\\\\cup.pni.princeton.edu\\braininit\\Data\\Raw\\electrophysiology\\jyanar\\jyanar_ya054\\20251010_g0\\ya054_20251010_g0\n",
      "nChan: 4, nFileSamp: 118289333\n"
     ]
    }
   ],
   "source": [
    "for idx_session in range(len(recording_keys)):\n",
    "    recording_key = recording_keys[idx_session]\n",
    "    session_key = session_keys[idx_session]\n",
    "\n",
    "    session_dir = find_full_path(get_ephys_root_data_dir(),\n",
    "                                get_session_directory(recording_key))\n",
    "    print(session_dir)\n",
    "    #session_dir = pathlib.Path('/Users/alvaros/Documents/MATLAB/BrainCogsProjects/CalciumImagingData/test_g0/')\n",
    "    #Check if session is Nidq or OneBox\n",
    "    nidq_session = list(session_dir.glob('*nidq.bin*'))\n",
    "    obx_session = list(session_dir.glob('*obx.bin*'))\n",
    "\n",
    "    if len(nidq_session) == 0 and len(obx_session) == 0:\n",
    "        print('No session found')\n",
    "    elif len(nidq_session) > 0:\n",
    "        ephys_session_fullpath = nidq_session[0]\n",
    "    else:\n",
    "        ephys_session_fullpath = obx_session[0]\n",
    "\n",
    "    #Nidaq file\n",
    "    nidq_meta          = readSGLX.readMeta(ephys_session_fullpath)\n",
    "    nidq_sampling_rate = readSGLX.SampRate(nidq_meta)\n",
    "\n",
    "\n",
    "    # 1: load meta data, and the content of the NIDAQ file. Its content is digital.            \n",
    "    new_trial_channel = 1\n",
    "    new_iteration_channel = 2\n",
    "    # If PXIe card (nidq) card use for recording deduce digital channels\n",
    "    if nidq_meta['typeThis'] == 'nidq':\n",
    "        digital_array      = ephys_utils.spice_glx_utility.load_spice_glx_digital_file(ephys_session_fullpath, nidq_meta)\n",
    "    # If onebox card (obx) card use for recording digital channels are 0-2\n",
    "    else:\n",
    "        digital_array      = ephys_utils.spice_glx_utility.load_spice_glx_digital_file(ephys_session_fullpath, nidq_meta, d_line_list=[0,1])\n",
    "        # If no sync pulse found trial and iteration signals are 0 & 1 respectively\n",
    "        channel0_pulses = np.where(np.diff(digital_array[0])==1)[0].shape[0]\n",
    "        channel1_pulses = np.where(np.diff(digital_array[1])==1)[0].shape[0]\n",
    "\n",
    "        if channel0_pulses > channel1_pulses:\n",
    "            new_trial_channel = 1\n",
    "            new_iteration_channel = 0\n",
    "        else:\n",
    "            new_trial_channel = 0\n",
    "            new_iteration_channel = 1\n",
    "\n",
    "    \n",
    "    thissession = behavior.TowersBlock().Trial() & session_key\n",
    "    behavior_time, iterstart, beh_num_iterations = thissession.fetch('trial_time', 'vi_start', 'iterations')\n",
    "\n",
    "    mode = None   #Default for sessions before 12/01/2021\n",
    "    #mode = 'pulses'    #Default for sessions after 12/01/2021\n",
    "    iteration_dict = ephys_utils.get_iteration_sample_vector_from_digital_lines_pulses(digital_array[new_trial_channel,:], digital_array[new_iteration_channel,:], nidq_sampling_rate, behavior_time.shape[0], behavior_time, mode=mode)\n",
    "    #                             get_iteration_sample_vector_from_digital_lines_pulses(trial_pulse_signal, iteration_pulse_signal,\n",
    "\n",
    "\n",
    "    extreme_cases = list()\n",
    "    ghost_iter = list()\n",
    "    ghost_iter2 = list()\n",
    "    for i in range(len(iteration_dict['iter_start_idx'])):\n",
    "\n",
    "        #print('fixing trial ',i)\n",
    "        behavior_time_vector = behavior_time[i].flatten()\n",
    "        synced_time_vector, shift_vec = get_shift_vector(iteration_dict['iter_times_idx'][i],behavior_time_vector)\n",
    "        synced_time_vector = fix_shifted_sync_vector(synced_time_vector, behavior_time_vector, shift_vec)\n",
    "\n",
    "        idx_ghost_iter = np.where(np.diff(synced_time_vector)<0)\n",
    "        idx_ghost_iter = idx_ghost_iter[0]\n",
    "        if idx_ghost_iter.shape[0] > 0:\n",
    "            ghost_iter.append(i)\n",
    "\n",
    "        synced_time_vector = fix_sync_vector_greater(synced_time_vector, behavior_time_vector)\n",
    "        synced_time_vector = complete_last_part_sync_vec(synced_time_vector, behavior_time_vector)\n",
    "\n",
    "        idx_ghost_iter = np.where(np.diff(synced_time_vector)<=0)\n",
    "        idx_ghost_iter = idx_ghost_iter[0]\n",
    "        if idx_ghost_iter.shape[0] > 0:\n",
    "            ghost_iter2.append(i)\n",
    "\n",
    "        synced_iteration_vector =\\\n",
    "            fix_iter_vector(iteration_dict['iter_start_idx'][i],synced_time_vector, iteration_dict['iter_times_idx'][i], nidq_sampling_rate)\n",
    "        \n",
    "\n",
    "        if iteration_dict['iter_times_idx'][i].shape[0] < behavior_time_vector.shape[0] - 20:\n",
    "            extreme_cases.append(i)\n",
    "\n",
    "        iteration_dict['iter_start_idx'][i] = synced_iteration_vector.copy()\n",
    "        iteration_dict['iter_times_idx'][i] = synced_time_vector.copy()\n",
    "        \n",
    "\n",
    "    # Check # of trials and iterations match\n",
    "    trial_count_diff, trials_diff_iteration_big, trials_diff_iteration_small = ephys_utils.assert_iteration_samples_count(iteration_dict['iter_start_idx'], behavior_time)\n",
    "\n",
    "    if trial_count_diff != 0:\n",
    "        print('trial_count_diff', trial_count_diff)\n",
    "    if len(trials_diff_iteration_big) > 0:\n",
    "        print('trials_diff_iteration_big', trials_diff_iteration_big)\n",
    "    if len(trials_diff_iteration_small) > 0:\n",
    "        print('trials_diff_iteration_small', trials_diff_iteration_small)\n",
    "\n",
    "\n",
    "    status = ephys_utils.evaluate_sync_process(trial_count_diff, trials_diff_iteration_big, trials_diff_iteration_small,  behavior_time.shape[0])\n",
    "\n",
    "    for i in range(len(iteration_dict['iter_start_idx'])):\n",
    "        synced_time_vector = iteration_dict['iter_times_idx'][i]\n",
    "        behavior_time_vector = behavior_time[i].flatten()\n",
    "\n",
    "        if i in ghost_iter:\n",
    "            print('Ghost iteration')\n",
    "            print(session_key)\n",
    "            print(recording_key)\n",
    "            print(i)\n",
    "        if i in ghost_iter2:\n",
    "            print('Ghost iteration 2')\n",
    "            print(session_key)\n",
    "            print(recording_key)\n",
    "            print(i)\n",
    "        if i in extreme_cases:\n",
    "            print('Too extreme to evaluate')\n",
    "            print(session_key)\n",
    "            print(recording_key)\n",
    "            print(i)\n",
    "\n",
    "        status = sync_evaluation_process2(synced_time_vector, behavior_time_vector)\n",
    "        if status == 0:\n",
    "            print(session_key)\n",
    "            print(recording_key)\n",
    "            print(i)\n",
    "            if i!=0:\n",
    "                break\n",
    "    \n",
    "    if status == 0:\n",
    "        break\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "u19_pipeline_python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
