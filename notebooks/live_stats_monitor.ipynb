{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate psychometrics plot given sessions & conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local configuration file found !!, no need to run the configuration (unless configuration has changed)\n"
     ]
    }
   ],
   "source": [
    "from scripts.conf_file_finding import try_find_conf_file\n",
    "try_find_conf_file()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luna_\\miniconda3\\envs\\u19_pipeline_python_env\\Lib\\site-packages\\datajoint\\plugin.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\luna_\\miniconda3\\envs\\u19_pipeline_python_env\\Lib\\site-packages\\datajoint\\plugin.py:4: UserWarning: Module scripts was already imported from None, but c:\\users\\luna_\\documents\\princetonprojects\\u19-pipeline-python is being added to sys.path\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import datajoint as dj\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import u19_pipeline.utils.slack_utils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datajoint configuration and Connection to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-17 13:37:00,431][INFO]: DataJoint 0.14.6 connected to alvaros@datajoint00.pni.princeton.edu:3306\n"
     ]
    }
   ],
   "source": [
    "dj.conn()\n",
    "\n",
    "MINUTES_ALERT = 20\n",
    "SECONDS_ALERT = MINUTES_ALERT*60\n",
    "MIN_SESSIONS_COMPLETED = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases to connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition = dj.create_virtual_module('acquisition', 'u19_acquisition')\n",
    "lab = dj.create_virtual_module('lab', 'u19_lab')\n",
    "subject = dj.create_virtual_module('subject', 'u19_subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_configuration_dictionary = {\n",
    "    'slack_notification_channel': ['alvaro_luna']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slack_alert_message_format_live_stats(alert_dictionary1, alert_dictionary2, time_no_response):\n",
    "\n",
    "    now = datetime.now()\n",
    "    datestr = now.strftime('%d-%b-%Y %H:%M:%S')\n",
    "\n",
    "    msep = dict()\n",
    "    msep['type'] = \"divider\"\n",
    "\n",
    "    #Title#\n",
    "    m1 = dict()\n",
    "    m1['type'] = 'section'\n",
    "    m1_1 = dict()\n",
    "    m1_1[\"type\"] = \"mrkdwn\"\n",
    "    m1_1[\"text\"] = ':rotating_light: * Live Monitor Alert* on ' + datestr + '\\n' +\\\n",
    "    'More than ' + str(int(time_no_response/60)) + ' min without valid new trial' + '\\n'\n",
    "    m1['text'] = m1_1\n",
    "\n",
    "    #Info#\n",
    "    m2 = dict()\n",
    "    m2['type'] = 'section'\n",
    "    m2_1 = dict()\n",
    "    m2_1[\"type\"] = \"mrkdwn\"\n",
    "\n",
    "    m2_1[\"text\"] = '*Session Reported:*' + '\\n'\n",
    "    for key in alert_dictionary1.keys():\n",
    "        m2_1[\"text\"] += '*' + key + '* : ' + str(alert_dictionary1[key]) + '\\n'\n",
    "    m2_1[\"text\"] += '\\n'\n",
    "    m2['text'] = m2_1\n",
    "\n",
    "    m4 = dict()\n",
    "    m4['type'] = 'section'\n",
    "    m4_1 = dict()\n",
    "    m4_1[\"type\"] = \"mrkdwn\"\n",
    "\n",
    "    m4_1[\"text\"] = '*Last Stats Reported*:' + '\\n'\n",
    "    for key in alert_dictionary2.keys():\n",
    "        m4_1[\"text\"] += '*' + key + '* : ' + str(alert_dictionary2[key]) + '\\n'\n",
    "    m4_1[\"text\"] += '\\n'\n",
    "    m4['text'] = m4_1\n",
    "\n",
    "    message = dict()\n",
    "    message['blocks'] = [m1,msep,m2,msep,m4,msep]\n",
    "    message['text'] = 'Live Monitor Alert'\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions STARTED RECENTLY\n",
      "   subject_fullname session_date  session_number  session_start_time  \\\n",
      "0    jeremyjc_j099   2025-11-17               0 2025-11-17 12:57:00   \n",
      "1         mjs20_35   2025-11-17               0 2025-11-17 14:05:00   \n",
      "2    testuser_1234   2025-11-17               1 2025-11-17 14:32:00   \n",
      "\n",
      "  session_location  \n",
      "0        185F-Rig1  \n",
      "1      170b-Rig1-I  \n",
      "2  165A-miniVR-T-8  \n",
      "Last session started on rig\n",
      "   subject_fullname session_date  session_number  session_start_time\n",
      "0    jeremyjc_j099   2025-11-17               0 2025-11-17 12:57:00\n",
      "1         mjs20_35   2025-11-17               0 2025-11-17 14:05:00\n",
      "2    testuser_1234   2025-11-17               1 2025-11-17 14:32:00\n",
      "Sessions not reported\n",
      "   subject_fullname session_date  session_number  session_start_time\n",
      "0    jeremyjc_j099   2025-11-17               0 2025-11-17 12:57:00\n",
      "1         mjs20_35   2025-11-17               0 2025-11-17 14:05:00\n",
      "Subjects with  completed min_num_sessions\n",
      "   subject_fullname session_date  session_number  session_start_time  \\\n",
      "0    jeremyjc_j099   2025-11-17               0 2025-11-17 12:57:00   \n",
      "1         mjs20_35   2025-11-17               0 2025-11-17 14:05:00   \n",
      "\n",
      "   num_sessions  \n",
      "0            90  \n",
      "1            18  \n"
     ]
    }
   ],
   "source": [
    "# Query today's started sessions that are not finished\n",
    "query = {}\n",
    "query['session_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "query['is_finished'] = 0\n",
    "\n",
    "#Only look for sessions started in the last 1:30\n",
    "last_time_start = datetime.now(tz=ZoneInfo('America/New_York')) - timedelta(hours=2,minutes=30)\n",
    "last_time_start = last_time_start.replace(tzinfo=None).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "query_started_recently = \"session_start_time > '\" + last_time_start + \"'\" \n",
    "sessions = pd.DataFrame((acquisition.SessionStarted & query & query_started_recently).fetch('KEY','session_location','session_start_time',as_dict=True))\n",
    "#sessions = sessions.loc[~sessions['subject_fullname'].str.startswith('testuser'),:]\n",
    "\n",
    "print('sessions STARTED RECENTLY\\n', sessions)\n",
    "\n",
    "if sessions.shape[0] > 0:\n",
    "\n",
    "    #If more than one \"not finished\" session in same rig, grab the last one started\n",
    "    sessions2 = sessions.groupby('session_location').agg({'session_start_time': [('session_start_time', 'max')]})\n",
    "    sessions2.columns = sessions2.columns.droplevel()\n",
    "    sessions2 = sessions2.reset_index()\n",
    "    sessions = pd.merge(sessions, sessions2, on=['session_location', 'session_start_time'])\n",
    "    sessions = sessions.drop(columns=['session_location'])\n",
    "    sessions = sessions.reset_index(drop=True)\n",
    "\n",
    "    print('Last session started on rig\\n', sessions)\n",
    "\n",
    "    #Only analyze sessions that have not been reported\n",
    "    query_reported  = {} \n",
    "    query_reported['session_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    sessions_reported = pd.DataFrame((acquisition.ReportedLiveSessionStats  & query_reported).fetch('KEY', as_dict = True))\n",
    "\n",
    "if sessions_reported.shape[0] > 0:\n",
    "\n",
    "    sessions = pd.merge(sessions,sessions_reported, how='left', indicator=True)\n",
    "    sessions = sessions.loc[sessions['_merge'] == 'left_only']\n",
    "    sessions = sessions.drop(columns='_merge')\n",
    "    sessions = sessions.reset_index(drop=True)\n",
    "\n",
    "print('Sessions not reported\\n', sessions)\n",
    "\n",
    "#Only analyze sessions subjects > NUM_SESSIONS_COMPLETED have not been reported\n",
    "if sessions.shape[0] > 0:\n",
    "\n",
    "    query_subjects = \"subject_fullname in ('\"+ \"', '\".join(sessions['subject_fullname']) + \"')\"\n",
    "\n",
    "    count_sessions_table = (subject.Subject).aggr((acquisition.Session & query_subjects), num_sessions=\"count(subject_fullname)\")\n",
    "    count_sessions_df = pd.DataFrame(count_sessions_table.fetch(as_dict=True))\n",
    "\n",
    "    sessions = pd.merge(sessions,count_sessions_df, how='left')\n",
    "    sessions['num_sessions'] = sessions['num_sessions'].fillna(0)\n",
    "\n",
    "    sessions = sessions.loc[sessions['num_sessions']>= MIN_SESSIONS_COMPLETED, :]\n",
    "\n",
    "print('Subjects with  completed min_num_sessions\\n', sessions)\n",
    "\n",
    "if sessions.shape[0] > 0:\n",
    "\n",
    "    # Query last live stat from the started sessions\n",
    "    query_live_stats = sessions.to_dict('records')\n",
    "    \n",
    "    #Last non violation trial in sessions\n",
    "    query_no_violation_trial = dict()\n",
    "    query_no_violation_trial['violation_trial'] = 0\n",
    "    lss_nvio = acquisition.SessionStarted.aggr((acquisition.LiveSessionStats & query_no_violation_trial).proj('current_datetime'), current_datetime=\"max(current_datetime)\")\n",
    "    live_stats_nvio = pd.DataFrame((lss_nvio & query_live_stats).fetch(as_dict=True))\n",
    "    live_stats_nvio = live_stats_nvio.rename({'current_datetime': 'last_non_violation_trial'}, axis=1)\n",
    "    \n",
    "    #Last violation trial in sessions\n",
    "    query_violation_trial = dict()\n",
    "    query_violation_trial['violation_trial'] = 1\n",
    "    lss_vio = acquisition.SessionStarted.aggr((acquisition.LiveSessionStats & query_violation_trial).proj('current_datetime'), current_datetime=\"max(current_datetime)\")\n",
    "    live_stats_vio = pd.DataFrame((lss_vio & query_live_stats & query_violation_trial).fetch(as_dict=True))\n",
    "    live_stats_vio = live_stats_vio.rename({'current_datetime': 'last_violation_trial'}, axis=1)\n",
    "\n",
    "\n",
    "    # Merge last violation trial and last non violation Trials from sessions\n",
    "    if live_stats_nvio.shape[0] > 0 and live_stats_vio.shape[0] > 0:\n",
    "        live_stats = pd.merge(live_stats_nvio,live_stats_vio, how='outer')\n",
    "    elif live_stats_nvio.shape[0] > 0:\n",
    "        live_stats = live_stats_nvio.copy()\n",
    "        live_stats['last_violation_trial'] = pd.NaT\n",
    "    elif live_stats_vio.shape[0] > 0:\n",
    "        live_stats = live_stats_vio.copy()\n",
    "        live_stats['last_non_violation_trial'] = pd.NaT\n",
    "    else:\n",
    "        live_stats = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # If there are any sessions with live stats\n",
    "    if live_stats.shape[0] > 0:\n",
    "\n",
    "        live_stats = pd.merge(sessions,live_stats, how='inner')\n",
    "        fake_date = pd.Timestamp('1900-01-01')\n",
    "\n",
    "        # Filter sessions whose last trial info is greater than 300s\n",
    "        right_now_est = datetime.now(tz=ZoneInfo('America/New_York'))\n",
    "        right_now_est = right_now_est.replace(tzinfo=None)\n",
    "        live_stats['seconds_elapsed_last_stat_nvio'] = (right_now_est- live_stats['last_non_violation_trial']).dt.total_seconds()\n",
    "        live_stats['alert_nvio'] = live_stats['seconds_elapsed_last_stat_nvio'] > SECONDS_ALERT\n",
    "\n",
    "        live_stats['seconds_elapsed_session_started'] = (right_now_est- live_stats['session_start_time']).dt.total_seconds()\n",
    "        live_stats['alert_vio'] = live_stats['seconds_elapsed_session_started'] > SECONDS_ALERT & pd.isna(live_stats['last_non_violation_trial'].isna()) & ~pd.isna(live_stats['last_violation_trial'].isna())\n",
    "\n",
    "\n",
    "        live_stats = live_stats.loc[(live_stats['alert_nvio']==True) | (live_stats['alert_vio']==True),:]\n",
    "\n",
    "        #If there are any sessions to alert (more then 300s)\n",
    "        if live_stats.shape[0] > 0:\n",
    "\n",
    "            live_stats['current_datetime'] = live_stats[['last_violation_trial', 'last_non_violation_trial']].fillna(fake_date).max(axis=1)\n",
    "            live_stats['seconds_elapsed_last_valid_stat'] = live_stats[['seconds_elapsed_last_stat_nvio', 'seconds_elapsed_session_started']].fillna(-np.inf).max(axis=1)\n",
    "\n",
    "            #get_session_info to alert (plus slack researcher)\n",
    "            query_live_stats_sessions = live_stats[['subject_fullname', 'session_date', 'session_number']].to_dict('records')\n",
    "\n",
    "            session_data_df = pd.DataFrame(((lab.User.proj('slack') * subject.Subject.proj('user_id') *\\\n",
    "                                        acquisition.SessionStarted.proj('session_location')) & query_live_stats_sessions).fetch(as_dict=True))\n",
    "            \n",
    "            session_data_df = session_data_df.rename({'slack': 'researcher'}, axis=1)\n",
    "            session_data_df['researcher'] = '<@'+ session_data_df['researcher'] + '>'\n",
    "            session_data_df = session_data_df[['researcher', 'subject_fullname', 'session_date', 'session_number']]\n",
    "\n",
    "            #Query full live stat table\n",
    "            #session_stats = live_stats.copy()\n",
    "            #session_stats = session_stats.rename({'current_datetime': 'last_live_stat'}, axis=1)\n",
    "            query_live_stats = live_stats[['subject_fullname', 'session_date', 'session_number', 'current_datetime']].to_dict('records')\n",
    "            live_stats_mini = live_stats[['subject_fullname', 'session_date', 'session_number', 'seconds_elapsed_last_valid_stat']].copy()\n",
    "            ls_full_df = pd.DataFrame((acquisition.LiveSessionStats & query_live_stats).fetch(as_dict=True))\n",
    "            ls_full_df = pd.merge(ls_full_df, live_stats_mini, on=['subject_fullname', 'session_date', 'session_number'])\n",
    "            ls_full_df = ls_full_df.drop(columns=['subject_fullname', 'session_date', 'session_number'])\n",
    "            ls_full_df = ls_full_df.rename({'current_datetime': 'last_trial_time'}, axis=1)\n",
    "\n",
    "\n",
    "            mid = ls_full_df['last_trial_time']\n",
    "            ls_full_df = ls_full_df.drop(columns=['last_trial_time'])\n",
    "            ls_full_df.insert(0, 'last_trial_time', mid)\n",
    "\n",
    "            ls_full_dict = ls_full_df.to_dict('records')\n",
    "\n",
    "            # Send one alert per session found\n",
    "            idx_alert = 0\n",
    "            for this_alert_record in ls_full_dict:\n",
    "\n",
    "                #Format message for session and live stat dictionary\n",
    "                this_session_stats = session_data_df.iloc[idx_alert,:]\n",
    "                slack_json_message = slack_alert_message_format_live_stats(this_session_stats.to_dict(), this_alert_record, int(this_alert_record['seconds_elapsed_last_valid_stat']))\n",
    "\n",
    "                #Send alert\n",
    "                webhooks_list = su.get_webhook_list(slack_configuration_dictionary, lab)\n",
    "                for this_webhook in webhooks_list:\n",
    "                    su.send_slack_notification(this_webhook, slack_json_message)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                reported_session = this_session_stats[['subject_fullname', 'session_date', 'session_number']].copy()\n",
    "                reported_session['report_datetime'] = right_now_est\n",
    "\n",
    "                acquisition.ReportedLiveSessionStats.insert1(reported_session.to_dict())\n",
    "                idx_alert += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u19_pipeline_python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
