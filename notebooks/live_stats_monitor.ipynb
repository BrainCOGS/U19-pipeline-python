{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate psychometrics plot given sessions & conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import pandas as pd\n",
    "import time\n",
    "from zoneinfo import ZoneInfo\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import u19_pipeline.utils.slack_utils as su\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datajoint configuration and Connection to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-08 11:49:25,841][INFO]: Connecting alvaros@datajoint00.pni.princeton.edu:3306\n",
      "[2025-05-08 11:49:27,625][INFO]: Connected alvaros@datajoint00.pni.princeton.edu:3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataJoint connection (connected) alvaros@datajoint00.pni.princeton.edu:3306"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases to connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition = dj.create_virtual_module('acquisition', 'u19_acquisition')\n",
    "lab = dj.create_virtual_module('lab', 'u19_lab')\n",
    "subject = dj.create_virtual_module('subject', 'u19_subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_configuration_dictionary = {\n",
    "    'slack_notification_channel': ['alvaro_luna']\n",
    "}\n",
    "webhooks_list = []\n",
    "query_slack_webhooks = [{'webhook_name' : x} for x in slack_configuration_dictionary['slack_notification_channel']]\n",
    "webhooks_list += (lab.SlackWebhooks & query_slack_webhooks).fetch('webhook_url').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slack_alert_message_format_live_stats(alert_dictionary1, alert_dictionary2, time_no_response):\n",
    "\n",
    "    now = datetime.now()\n",
    "    datestr = now.strftime('%d-%b-%Y %H:%M:%S')\n",
    "\n",
    "    msep = dict()\n",
    "    msep['type'] = \"divider\"\n",
    "\n",
    "    #Title#\n",
    "    m1 = dict()\n",
    "    m1['type'] = 'section'\n",
    "    m1_1 = dict()\n",
    "    m1_1[\"type\"] = \"mrkdwn\"\n",
    "    m1_1[\"text\"] = ':rotating_light: * Live Monitor Alert* on ' + datestr + '\\n' +\\\n",
    "    'More than ' + str(time_no_response) + ' s without new trial' + '\\n'\n",
    "    m1['text'] = m1_1\n",
    "\n",
    "    #Info#\n",
    "    m2 = dict()\n",
    "    m2['type'] = 'section'\n",
    "    m2_1 = dict()\n",
    "    m2_1[\"type\"] = \"mrkdwn\"\n",
    "\n",
    "    m2_1[\"text\"] = '*Session Reported:*' + '\\n'\n",
    "    for key in alert_dictionary1.keys():\n",
    "        m2_1[\"text\"] += '*' + key + '* : ' + str(alert_dictionary1[key]) + '\\n'\n",
    "    m2_1[\"text\"] += '\\n'\n",
    "    m2['text'] = m2_1\n",
    "\n",
    "    m4 = dict()\n",
    "    m4['type'] = 'section'\n",
    "    m4_1 = dict()\n",
    "    m4_1[\"type\"] = \"mrkdwn\"\n",
    "\n",
    "    m4_1[\"text\"] = '*Last Stats Reported*:' + '\\n'\n",
    "    for key in alert_dictionary2.keys():\n",
    "        m4_1[\"text\"] += '*' + key + '* : ' + str(alert_dictionary2[key]) + '\\n'\n",
    "    m4_1[\"text\"] += '\\n'\n",
    "    m4['text'] = m4_1\n",
    "\n",
    "    message = dict()\n",
    "    message['blocks'] = [m1,msep,m2,msep,m4,msep]\n",
    "    message['text'] = 'Live Monitor Alert'\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions STARTED RECENTLY\n",
      "         subject_fullname session_date  session_number  session_start_time  \\\n",
      "0  efonseca_ef757_act124   2025-05-08               0 2025-05-08 11:55:00   \n",
      "1  efonseca_ef757_act124   2025-05-08               1 2025-05-08 11:58:00   \n",
      "2          jeremyjc_j084   2025-05-08               0 2025-05-08 13:51:00   \n",
      "3            jk8386_jk75   2025-05-08               0 2025-05-08 13:51:00   \n",
      "4           jyanar_ya026   2025-05-08               0 2025-05-08 13:51:00   \n",
      "5           jyanar_ya027   2025-05-08               0 2025-05-08 13:51:00   \n",
      "6              mjs20_481   2025-05-08               0 2025-05-08 13:52:00   \n",
      "\n",
      "  session_location  \n",
      "0  165A-miniVR-T-1  \n",
      "1  165A-miniVR-T-1  \n",
      "2      165I-Rig1-T  \n",
      "3      165I-Rig4-T  \n",
      "4      165I-Rig2-T  \n",
      "5      165I-Rig3-T  \n",
      "6      170b-Rig1-I  \n",
      "Last session started on rig\n",
      "         subject_fullname session_date  session_number\n",
      "0  efonseca_ef757_act124   2025-05-08               1\n",
      "1          jeremyjc_j084   2025-05-08               0\n",
      "2            jk8386_jk75   2025-05-08               0\n",
      "3           jyanar_ya026   2025-05-08               0\n",
      "4           jyanar_ya027   2025-05-08               0\n",
      "5              mjs20_481   2025-05-08               0\n",
      "Sessions not reported\n",
      "   subject_fullname session_date  session_number\n",
      "0    jeremyjc_j084   2025-05-08               0\n",
      "1      jk8386_jk75   2025-05-08               0\n",
      "2     jyanar_ya026   2025-05-08               0\n",
      "3     jyanar_ya027   2025-05-08               0\n",
      "4        mjs20_481   2025-05-08               0\n"
     ]
    }
   ],
   "source": [
    "# Query today's started sessions that are not finished\n",
    "query = {}\n",
    "query['session_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "query['is_finished'] = 0\n",
    "\n",
    "#Only look for sessions started in the last 1:30\n",
    "last_time_start = datetime.now(tz=ZoneInfo('America/New_York')) - timedelta(hours=2,minutes=30)\n",
    "last_time_start = last_time_start.replace(tzinfo=None).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "query_started_recently = \"session_start_time > '\" + last_time_start + \"'\" \n",
    "sessions = pd.DataFrame((acquisition.SessionStarted & query & query_started_recently).fetch('KEY','session_location','session_start_time',as_dict=True))\n",
    "sessions = sessions.loc[~sessions['subject_fullname'].str.startswith('testuser'),:]\n",
    "\n",
    "print('sessions STARTED RECENTLY\\n', sessions)\n",
    "\n",
    "if sessions.shape[0] > 0:\n",
    "\n",
    "    #If more than one \"not finished\" session in same rig, grab the last one started\n",
    "    sessions2 = sessions.groupby('session_location').agg({'session_start_time': [('session_start_time', 'max')]})\n",
    "    sessions2.columns = sessions2.columns.droplevel()\n",
    "    sessions2 = sessions2.reset_index()\n",
    "    sessions = pd.merge(sessions, sessions2, on=['session_location', 'session_start_time'])\n",
    "    sessions = sessions.drop(columns=['session_location', 'session_start_time'])\n",
    "    sessions = sessions.reset_index(drop=True)\n",
    "\n",
    "    print('Last session started on rig\\n', sessions)\n",
    "\n",
    "    #Only analyze sessions that have not been reported\n",
    "    query_reported  = {} \n",
    "    query_reported['session_date'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    sessions_reported = pd.DataFrame((acquisition.ReportedLiveSessionStats  & query_reported).fetch('KEY', as_dict = True))\n",
    "\n",
    "    if sessions_reported.shape[0] > 0:\n",
    "\n",
    "        sessions = pd.merge(sessions,sessions_reported, how='left', indicator=True)\n",
    "        sessions = sessions.loc[sessions['_merge'] == 'left_only']\n",
    "        sessions = sessions.drop(columns='_merge')\n",
    "        sessions = sessions.reset_index(drop=True)\n",
    "\n",
    "    print('Sessions not reported\\n', sessions)\n",
    "\n",
    "if sessions.shape[0] > 0:\n",
    "\n",
    "    # Query last live stat from the started sessions\n",
    "    query_live_stats = sessions.to_dict('records')\n",
    "    lss = acquisition.SessionStarted.aggr(acquisition.LiveSessionStats.proj('current_datetime'), current_datetime=\"max(current_datetime)\")\n",
    "    live_stats = pd.DataFrame((lss & query_live_stats).fetch(as_dict=True))\n",
    "\n",
    "    # If there are any sessions with live stats\n",
    "    if live_stats.shape[0] > 0:\n",
    "\n",
    "        # Filter sessions whose last trial info is greater than 300s\n",
    "        right_now_est = datetime.now(tz=ZoneInfo('America/New_York'))\n",
    "        right_now_est = right_now_est.replace(tzinfo=None)\n",
    "        live_stats['seconds_elapsed_last_stat'] = (right_now_est- live_stats['current_datetime']).dt.total_seconds()\n",
    "        live_stats['alert'] = live_stats['seconds_elapsed_last_stat'] >8    \n",
    "        live_stats = live_stats.loc[live_stats['alert']==True,:]\n",
    "\n",
    "\n",
    "        #If there are any sessions to alert (more then 300s)\n",
    "        if live_stats.shape[0] > 0:\n",
    "\n",
    "            #get_session_info to alert (plus slack researcher)\n",
    "            query_live_stats_sessions = live_stats[['subject_fullname', 'session_date', 'session_number']].to_dict('records')\n",
    "\n",
    "            session_data_df = pd.DataFrame(((lab.User.proj('slack') * subject.Subject.proj('user_id') *\\\n",
    "                                        acquisition.SessionStarted.proj('session_location')) & query_live_stats_sessions).fetch(as_dict=True))\n",
    "            \n",
    "            session_data_df = session_data_df.rename({'slack': 'researcher'}, axis=1)\n",
    "            session_data_df['researcher'] = '<@'+ session_data_df['researcher'] + '>'\n",
    "            session_data_df = session_data_df[['researcher', 'subject_fullname', 'session_date', 'session_number']]\n",
    "\n",
    "            #Query full live stat table\n",
    "            session_stats = live_stats.copy()\n",
    "            session_stats = session_stats.rename({'current_datetime': 'last_live_stat'}, axis=1)\n",
    "            query_live_stats = live_stats[['subject_fullname', 'session_date', 'session_number', 'current_datetime']].to_dict('records')\n",
    "            live_stats = live_stats.drop(columns=['current_datetime', 'alert'])\n",
    "            ls_full_df = pd.DataFrame((acquisition.LiveSessionStats & query_live_stats).fetch(as_dict=True))\n",
    "            ls_full_df = pd.merge(ls_full_df, live_stats, on=['subject_fullname', 'session_date', 'session_number'])\n",
    "            ls_full_df = ls_full_df.drop(columns=['subject_fullname', 'session_date', 'session_number'])\n",
    "            ls_full_df = ls_full_df.rename({'current_datetime': 'last_stat_time'}, axis=1)\n",
    "\n",
    "            mid = ls_full_df['last_stat_time']\n",
    "            ls_full_df = ls_full_df.drop(columns=['last_stat_time'])\n",
    "            ls_full_df.insert(0, 'last_stat_time', mid)\n",
    "\n",
    "            ls_full_dict = ls_full_df.to_dict('records')\n",
    "\n",
    "            # Send one alert per session found\n",
    "            idx_alert = 0\n",
    "            for this_alert_record in ls_full_dict:\n",
    "\n",
    "                #Format message for session and live stat dictionary\n",
    "                this_session_stats = session_data_df.iloc[idx_alert,:]\n",
    "                slack_json_message = slack_alert_message_format_live_stats(this_session_stats.to_dict(), this_alert_record, int(this_alert_record['seconds_elapsed_last_stat']))\n",
    "\n",
    "                #Send alert\n",
    "                for this_webhook in webhooks_list:\n",
    "                    su.send_slack_notification(this_webhook, slack_json_message)\n",
    "                    time.sleep(1)\n",
    "\n",
    "                reported_session = this_session_stats[['subject_fullname', 'session_date', 'session_number']].copy()\n",
    "                reported_session['report_datetime'] = right_now_est\n",
    "\n",
    "                acquisition.ReportedLiveSessionStats.insert1(reported_session.to_dict())\n",
    "                idx_alert += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 5, 8, 12, 17, 13, 541205)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u19_datajoint_py311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
